{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "**IMDB review** dataset is collection of sentences indicating positive and negative reviews with format {Sentence \\t Score \\n}. P.S. The dataset is taken from 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015 \n",
    "## Task\n",
    "Classifying a given IMDB reviews, as carrying positive (+1) or negative (0) sentiment toward the mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dYGZgACMLIzv"
   },
   "outputs": [],
   "source": [
    "# Opening and Reading the files into a list \n",
    "with open(\"imdb_labelled.txt\",\"r\") as text_file:\n",
    "    lines = text_file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "hgm-GNg5LLku",
    "outputId": "46f8f7a1-1c0e-4e74-db21-4cd7a2a1bf4a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \\t0',\n",
       " 'Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  \\t0',\n",
       " 'Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.  \\t0',\n",
       " 'Very little music or anything to speak of.  \\t0',\n",
       " 'The best scene in the movie was when Gerardo is trying to find a song that keeps running through his head.  \\t1',\n",
       " \"The rest of the movie lacks art, charm, meaning... If it's about emptiness, it works I guess because it's empty.  \\t0\",\n",
       " 'Wasted two hours.  \\t0',\n",
       " 'Saw the movie today and thought it was a good effort, good messages for kids.  \\t1',\n",
       " 'A bit predictable.  \\t0',\n",
       " 'Loved the casting of Jimmy Buffet as the science teacher.  \\t1']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the line by new-line character such that each line has one element of the list\n",
    "lines[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XSPU0UGgLNbe"
   },
   "outputs": [],
   "source": [
    "# Read the lines from both the files and append in same list\n",
    "with open(\"yelp_labelled.txt\",\"r\") as text_file:\n",
    "    lines += text_file.read().split('\\n')\n",
    "with open(\"amazon_cells_labelled.txt\",\"r\") as text_file:\n",
    "    lines += text_file.read().split('\\n')\n",
    "# print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHDXTPqALQNO"
   },
   "outputs": [],
   "source": [
    "# split by tab and remove corrupted data if any or lines which are not tab seperated\n",
    "lines = [line.split(\"\\t\") for line in lines if len(line.split(\"\\t\"))==2 and line.split(\"\\t\")[1]!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "XrFtNWq5LST-",
    "outputId": "155ab383-d4d9-4802-9f4c-99354915935a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  ',\n",
       "  '0'],\n",
       " ['Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  ',\n",
       "  '0'],\n",
       " ['Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.  ',\n",
       "  '0'],\n",
       " ['Very little music or anything to speak of.  ', '0'],\n",
       " ['The best scene in the movie was when Gerardo is trying to find a song that keeps running through his head.  ',\n",
       "  '1'],\n",
       " [\"The rest of the movie lacks art, charm, meaning... If it's about emptiness, it works I guess because it's empty.  \",\n",
       "  '0'],\n",
       " ['Wasted two hours.  ', '0'],\n",
       " ['Saw the movie today and thought it was a good effort, good messages for kids.  ',\n",
       "  '1'],\n",
       " ['A bit predictable.  ', '0'],\n",
       " ['Loved the casting of Jimmy Buffet as the science teacher.  ', '1']]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the lines one is string and another is integer 0 or 1\n",
    "lines[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "7Cp2LafdLUBp",
    "outputId": "4e25be87-5aab-4a1f-8f43-5fea0375a9eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  ',\n",
       " 'Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  ',\n",
       " 'Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.  ',\n",
       " 'Very little music or anything to speak of.  ',\n",
       " 'The best scene in the movie was when Gerardo is trying to find a song that keeps running through his head.  ',\n",
       " \"The rest of the movie lacks art, charm, meaning... If it's about emptiness, it works I guess because it's empty.  \",\n",
       " 'Wasted two hours.  ',\n",
       " 'Saw the movie today and thought it was a good effort, good messages for kids.  ',\n",
       " 'A bit predictable.  ',\n",
       " 'Loved the casting of Jimmy Buffet as the science teacher.  ']"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seperate the sentences\n",
    "train_documents = [line[0] for line in lines]\n",
    "train_documents[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4jL0M-ZaLVCJ",
    "outputId": "77e273e6-8794-4dd6-983d-5f0d9a290214"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 0, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seperate the labels\n",
    "train_labels = [int(line[1]) for line in lines]\n",
    "train_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5r8To0LLLYkJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuOZeIn1LZWR"
   },
   "outputs": [],
   "source": [
    "# Instatiate the Countvectorizer\n",
    "count_vectorizer = CountVectorizer(binary='true')\n",
    "# Train the documents\n",
    "train_documents_count = count_vectorizer.fit_transform(train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T-XJBrs5_L1f"
   },
   "outputs": [],
   "source": [
    "# Instatiate the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(binary='true')\n",
    "# Train the documents\n",
    "train_documents_tfidf = tfidf_vectorizer.fit_transform(train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "AiIkELExLajJ",
    "outputId": "375c0dfa-14fa-4a20-f400-6223e6b0fbb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2764)\t1\n",
      "  (0, 5139)\t1\n",
      "  (0, 1401)\t1\n",
      "  (0, 1331)\t1\n",
      "  (0, 75)\t1\n",
      "  (0, 2954)\t1\n",
      "  (0, 166)\t1\n",
      "  (0, 2956)\t1\n",
      "  (0, 4133)\t1\n",
      "  (0, 4890)\t1\n",
      "  (0, 4890)\t0.1705983714180228\n",
      "  (0, 4133)\t0.2866535155030133\n",
      "  (0, 2956)\t0.344069060569113\n",
      "  (0, 166)\t0.39646013048660145\n",
      "  (0, 2954)\t0.18376294855720401\n",
      "  (0, 75)\t0.21995093564903362\n",
      "  (0, 1331)\t0.39646013048660145\n",
      "  (0, 1401)\t0.39646013048660145\n",
      "  (0, 5139)\t0.3527636851677853\n",
      "  (0, 2764)\t0.303662775383371\n"
     ]
    }
   ],
   "source": [
    "# print(train_documents)\n",
    "\n",
    "# print first document\n",
    "print(train_documents_count[0])\n",
    "print(train_documents_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOQv4n6VLb6y"
   },
   "outputs": [],
   "source": [
    "# Training Phase\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "count_classifier = BernoulliNB().fit(train_documents_count, train_labels)\n",
    "tfidf_classifier = BernoulliNB().fit(train_documents_tfidf, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CapoSvdlLfjL"
   },
   "outputs": [],
   "source": [
    "# Test Phase\n",
    "# Opening and Reading the test file into a list \n",
    "import xlrd\n",
    "\n",
    "workbook = xlrd.open_workbook(\"yelp-test.xlsx\")\n",
    "sheet = workbook.sheet_by_index(1)  #iki sheet var. Bizimki ikincisi olduğundan index 1\n",
    "\n",
    "test_documents = list()\n",
    "for row in range(sheet.nrows):\n",
    "\ttest_documents.append(sheet.cell(row,0).value)\n",
    "\t#print(sheet.cell(row, 0).value)\n",
    "\n",
    "test_labels = list()\n",
    "for row in range(sheet.nrows):\n",
    "\ttest_labels.append(sheet.cell(row,1).value)\n",
    "\t#print(sheet.cell(row, 1).value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "iLCBHJMXK8mb",
    "outputId": "bb8c19b0-774a-4cea-97a3-07e7eb8d75a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer Accuracy Score:  0.5643564356435643\n",
      "Tf-Idf Vectorizer Accuracy Score:  0.5643564356435643\n"
     ]
    }
   ],
   "source": [
    "# Making the predictions\n",
    "pred_count = count_classifier.predict(count_vectorizer.transform(test_documents))\n",
    "pred_tfidf = tfidf_classifier.predict(tfidf_vectorizer.transform(test_documents))\n",
    "\n",
    "# Print the accuracy scores\n",
    "print(\"Count Vectorizer Accuracy Score: \", count_classifier.score((count_vectorizer.transform(test_documents)), test_labels))\n",
    "print(\"Tf-Idf Vectorizer Accuracy Score: \", tfidf_classifier.score((tfidf_vectorizer.transform(test_documents)), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sScbBbXrVG8E"
   },
   "outputs": [],
   "source": [
    "test_labels = numpy.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "OCPdgtbTM2qB",
    "outputId": "7046681d-e676-414a-8fdd-c5a54a506f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35 30]\n",
      " [14 22]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.54      0.61        65\n",
      "           1       0.42      0.61      0.50        36\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       101\n",
      "   macro avg       0.57      0.57      0.56       101\n",
      "weighted avg       0.61      0.56      0.57       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the confusion matrix and classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(test_labels, pred_count))\n",
    "print('\\n')\n",
    "print(classification_report(test_labels, pred_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "0hmYr6t__1WF",
    "outputId": "a9dcdf32-f52e-4cf9-9c52-6a83053b06f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35 30]\n",
      " [14 22]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.54      0.61        65\n",
      "           1       0.42      0.61      0.50        36\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       101\n",
      "   macro avg       0.57      0.57      0.56       101\n",
      "weighted avg       0.61      0.56      0.57       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the confusion matrix and classification report\n",
    "print(confusion_matrix(test_labels, pred_tfidf))\n",
    "print('\\n')\n",
    "print(classification_report(test_labels, pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mp8EmjDiA9VT"
   },
   "source": [
    "## Summary\n",
    "\n",
    "For this problem, we have used the Count Vectorizer and TF-IDF Vectorizer algorithms to learn the vocabulary. They have produced the same results. Then, we attempted to use k-fold cross-validation with our training data. The generated models provided no improvement, thus we have omitted them from the notebook for clarity. The results can be seen above."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cs412project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
