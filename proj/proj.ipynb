{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhttps://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\\nhttps://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0\\nhttps://towardsdatascience.com/feature-engineering-in-python-part-i-the-most-powerful-way-of-dealing-with-data-8e2447e7c69e\\nhttps://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier,GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pprint import pprint\n",
    "'''\n",
    "https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n",
    "https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0\n",
    "https://towardsdatascience.com/feature-engineering-in-python-part-i-the-most-powerful-way-of-dealing-with-data-8e2447e7c69e\n",
    "https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./loansTest.csv\")\n",
    "train = pd.read_csv(\"./loansTrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature columns\n",
    "feature_cols = list(train.columns[:-1])\n",
    "# extract target column 'loan_status'\n",
    "target_col = train.columns[-1]\n",
    "\n",
    "# separate the data into feature data and target data\n",
    "X_all = train[feature_cols]\n",
    "y_all = train[target_col]\n",
    "\n",
    "# extract feature columns\n",
    "feature_cols = list(test.columns[1:])\n",
    "# extract target column 'loan_status'\n",
    "target_col = test.columns[0]\n",
    "\n",
    "# separate the data into feature data and target data\n",
    "X_test = test[feature_cols]\n",
    "id_test = test[target_col]\n",
    "\n",
    "X_all = pd.concat([X_all, X_test], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(X):\n",
    "    ''' Converts categorical variables into dummy variables. '''\n",
    "    \n",
    "    # Initialize new output DataFrame\n",
    "    output = pd.DataFrame(index = X.index)\n",
    "\n",
    "    # Investigate each feature column for the data\n",
    "    for col, col_data in X.iteritems():\n",
    "        print(col)\n",
    "        # If data type is categorical, convert to dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            # Example: 'school' => 'school_GP' and 'school_MS'\n",
    "            col_data = pd.get_dummies(col_data, prefix = col)  \n",
    "        \n",
    "        # Collect the revised columns\n",
    "        output = output.join(col_data)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toBeTransform = ['emp_length','mo_sin_old_rev_tl_op','mo_sin_rcnt_rev_tl_op','mo_sin_rcnt_tl','mths_since_recent_bc','mths_since_recent_inq']\n",
    "# to help normalizing the magnitude diffrences in time related features\n",
    "# for t in toBeTransform:\n",
    "#     X_all[t] = (X_all[t]+1).transform(np.log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that it contains categorical values. After using X_all['emp_title'].unique() , we notice that the values of the emp_title are repeated in a way. So we are trying to combine repeated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................"
     ]
    }
   ],
   "source": [
    "# all missing values unemployed\n",
    "X_all['emp_title'].fillna(\"unemployed\",inplace=True)\n",
    "\n",
    "# all titles lower case\n",
    "X_all['emp_title'].str.lower()\n",
    "\n",
    "# Replace non-alphanumeric\n",
    "X_all['emp_title'] = X_all['emp_title'].astype(str).replace('[^a-zA-Z0-9 ]', '', regex=True)\n",
    "\n",
    "# fix spelling mistakes\n",
    "#X_all['emp_title'].apply(lambda txt: ''.join(TextBlob(txt).correct()))\n",
    "\n",
    "frequent_titles = pd.Series(' '.join(X_all['emp_title']).lower().split()).value_counts()[:100]\n",
    "arr = frequent_titles.index.to_numpy()\n",
    "\n",
    "to_be_ignored = ['the', 'of', 'and', '&', 'city', '-', '/','sr.', 'sr','inc.']\n",
    "\n",
    "for ingnore in to_be_ignored:\n",
    "    arr = np.delete(arr, np.argwhere(arr == ingnore))\n",
    "\n",
    "# Feature Transformation\n",
    "\n",
    "whole = '|'.join(arr)\n",
    "arr = map(lambda x:[x, re.compile('.*'+ x +'.*')], arr )\n",
    "\n",
    "for r in arr:\n",
    "    print('.',end=\"\")\n",
    "    X_all['emp_title'] = X_all['emp_title'].str.replace(r[1], r[0], regex=True)\n",
    "    \n",
    "# renaming everything else to 'other'\n",
    "X_all['emp_title'] = np.where(~X_all['emp_title'].str.contains(whole), 'other', X_all['emp_title'])\n",
    "\n",
    "\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "X_all = X_all[X_all.columns[X_all.isnull().mean() < 0.7]]\n",
    "\n",
    "# replace null with zeros\n",
    "for col in X_all.columns:\n",
    "    # Replacing the missing values with the maximum occurred value \n",
    "    X_all[col].fillna(X_all[col].value_counts().idxmax(), inplace=True)\n",
    "    #X_all[col].fillna(0,inplace=True) \n",
    "    \n",
    "    # removing trailing whitespaces and making all lowercase\n",
    "    if X_all[col].dtype == np.object:\n",
    "        X_all[col].str.strip()\n",
    "        X_all[col].str.lower()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech', 'it', 'unemployed', 'other', 'manager', 'vice', 'teacher',\n",
       "       'marketing', 'mechanic', 'therapist', 'inc', 'operator', 'project',\n",
       "       'assistant', 'developer', 'sales', 'supervisor', 'driver', 'owner',\n",
       "       'rep', 'analyst', 'rn', 'specialist', 'truck', 'director',\n",
       "       'office', 'admin', 'maintenance', 'engineer', 'executive', 'lead',\n",
       "       'senior', 'development', 'customer', 'warehouse', 'nurse',\n",
       "       'account', 'general', 'worker', 'president', 'center', 'care',\n",
       "       'coordinator', 'registered', 'systems', 'department', 'asst',\n",
       "       'agent', 'clerk', 'consultant', 'field', 'school', 'foreman',\n",
       "       'store', 'software', 'operations', 'ii', 'associate', 'production',\n",
       "       'medical', 'program', 'health', 'bank', 'clinical', 'advisor',\n",
       "       'support', 'management', 'hr', 'finance', 'county', 'vp',\n",
       "       'district', 'chief', 'staff', 'team', 'police', 'financial',\n",
       "       'network', 'loan', 'business', 'regional', 'branch'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all['emp_title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature removel\n",
    "# X_all.drop(['zip_code', 'addr_state', 'earliest_cr_line'], axis=1, inplace=True)\n",
    "# feature removel\n",
    "# X_all.drop(['zip_code', 'dti', 'avg_cur_bal','application_type','emp_title','chargeoff_within_12_mths','acc_now_delinq', 'delinq_amnt','percent_bc_gt_75','num_tl_120dpd_2m','num_tl_30dpd','num_tl_90g_dpd_24m','tax_liens'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variable into dummy/indicator variables\n",
    "X_all['term'] = X_all['term'].replace(['36 months', '60 months'], [0, 1])\n",
    "X_all['term'] = X_all['term'].replace([' 36 months', ' 60 months'], [0, 1])\n",
    "X_all['verification_status'] = pd.get_dummies(X_all['verification_status'], prefix = 'verification_status')  \n",
    "X_all['purpose'] = pd.get_dummies(X_all['purpose'], prefix = 'purpose')  \n",
    "X_all['application_type'] = pd.get_dummies(X_all['application_type'], prefix = 'application_type')  \n",
    "X_all['emp_title'] = pd.get_dummies(X_all['emp_title'], prefix = 'emp_title')  \n",
    "\n",
    "X_all.drop(['zip_code', 'dti', 'avg_cur_bal','application_type','chargeoff_within_12_mths','acc_now_delinq', 'delinq_amnt','percent_bc_gt_75','num_tl_120dpd_2m','num_tl_30dpd','num_tl_90g_dpd_24m','tax_liens'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_all.drop(['application_type','emp_title','chargeoff_within_12_mths','acc_now_delinq', 'delinq_amnt','percent_bc_gt_75','num_tl_120dpd_2m','num_tl_30dpd','num_tl_90g_dpd_24m','tax_liens'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Selection\n",
    "Statistical tests can be used to select those features that have the strongest relationship with the output variable. Chi-squared (chi²) statistical test is being used to select 10 of the best features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import chi2\n",
    "\n",
    "# #apply SelectKBest class to extract top 10 best features\n",
    "# bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "# fit = bestfeatures.fit(X_train,y_train)\n",
    "# dfscores = pd.DataFrame(fit.scores_)\n",
    "# dfcolumns = pd.DataFrame(X_train.columns)\n",
    "# #concat two dataframes for better visualization \n",
    "# featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "# featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "# print(featureScores.nlargest(20,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importence\n",
    "Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# model = ExtraTreesClassifier()\n",
    "# model.fit(X_train,y_train)\n",
    "# print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "# #plot graph of feature importances for better visualization\n",
    "# feat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "# feat_importances.nlargest(20).plot(kind='barh')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # len(feat_importances.index)\n",
    "# len(featureScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix with Heatmap\n",
    "Correlation states how the features are related to each other or the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# #get correlations of each features in dataset\n",
    "# corrmat = train.corr()\n",
    "# top_corr_features = corrmat.index\n",
    "# plt.figure(figsize=(20,20))\n",
    "# #plot heat map\n",
    "# g=sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collinear Features\n",
    "Collinear features are features that are highly correlated with one another. In machine learning, these lead to decreased generalization performance on the test set due to high variance and less model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create correlation matrix\n",
    "# corr_matrix = X_all.corr().abs()\n",
    "\n",
    "# # Select upper triangle of correlation matrix\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# # Find index of feature columns with correlation greater than 0.90\n",
    "# to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "\n",
    "# X_all.drop(X_all[to_drop], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Importence Features\n",
    "The identify_zero_importance function finds features that have zero importance according to a gradient boosting machine (GBM) learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pass in the appropriate parameters\n",
    "# fs.identify_zero_importance(task = 'classification', \n",
    "#                             eval_metric = 'auc', \n",
    "#                             n_iterations = 10, \n",
    "#                              early_stopping = True)\n",
    "# # list of zero importance features\n",
    "# zero_importance_features = fs.ops['zero_importance']\n",
    "# # plot the feature importances\n",
    "# fs.plot_feature_importances(threshold = 0.99, plot_n = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs.identify_low_importance(cumulative_importance = 0.99)\n",
    "# fs.feature_importances.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Unique Value Features\n",
    "find any columns that have a single unique value. A feature with only one unique value cannot be useful for machine learning because this feature has zero variance. For example, a tree-based model can never make a split on a feature with only one value (since there are no groups to divide the observations into)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs.identify_single_unique()\n",
    "# fs.plot_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# h = .02  # step size in the mesh\n",
    "\n",
    "# names = [ #\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "#          \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\"RandomForestRegresor\"]\n",
    "\n",
    "# classifiers = [\n",
    "# #     KNeighborsClassifier(3),\n",
    "# #     SVC(kernel=\"linear\", C=0.025),\n",
    "# #     SVC(gamma=2, C=1),\n",
    "# #     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     DecisionTreeClassifier(max_depth=5),\n",
    "#     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "#     MLPClassifier(alpha=1, max_iter=1000),\n",
    "#     AdaBoostClassifier(n_estimators=100, random_state=0),\n",
    "#     RandomForestRegressor(max_depth=5, n_estimators=10, max_features=1)]\n",
    "\n",
    "# # iterate over classifiers\n",
    "# for name, clf in zip(names, classifiers):\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     score = clf.score(X_val, y_val)\n",
    "#     print(name, \":\", score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_amnt\n",
      "term\n",
      "grade\n",
      "emp_title\n",
      "emp_length\n",
      "home_ownership\n",
      "annual_inc\n",
      "verification_status\n",
      "purpose\n",
      "addr_state\n",
      "delinq_2yrs\n",
      "earliest_cr_line\n",
      "inq_last_6mths\n",
      "open_acc\n",
      "pub_rec\n",
      "revol_bal\n",
      "revol_util\n",
      "collections_12_mths_ex_med\n",
      "tot_coll_amt\n",
      "tot_cur_bal\n",
      "total_rev_hi_lim\n",
      "acc_open_past_24mths\n",
      "bc_open_to_buy\n",
      "bc_util\n",
      "mo_sin_old_rev_tl_op\n",
      "mo_sin_rcnt_rev_tl_op\n",
      "mo_sin_rcnt_tl\n",
      "mort_acc\n",
      "mths_since_recent_bc\n",
      "mths_since_recent_inq\n",
      "num_accts_ever_120_pd\n",
      "num_actv_bc_tl\n",
      "num_actv_rev_tl\n",
      "num_bc_sats\n",
      "num_bc_tl\n",
      "num_il_tl\n",
      "num_op_rev_tl\n",
      "num_rev_accts\n",
      "num_rev_tl_bal_gt_0\n",
      "num_sats\n",
      "num_tl_op_past_12m\n",
      "pct_tl_nvr_dlq\n",
      "pub_rec_bankruptcies\n",
      "tot_hi_cred_lim\n",
      "total_bal_ex_mort\n",
      "total_il_high_credit_limit\n"
     ]
    }
   ],
   "source": [
    "X_all = preprocess_features(X_all)\n",
    "X, X_test = X_all.iloc[:1000000,:], X_all.iloc[1000000:,:]\n",
    "\n",
    "# newX =  X_all.iloc[:10000,:]\n",
    "# newY =  y_all.iloc[:10000]\n",
    "# X_train, X_val, y_train, y_val = train_test_split(newX, newY, train_size=0.95, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_all, train_size=0.95, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.feature_selection import RFE\n",
    "\n",
    "# cols = list(X_train.columns)\n",
    "# model = LinearRegression()\n",
    "# #Initializing RFE model\n",
    "# rfe = RFE(model, 10)             \n",
    "# #Transforming data using RFE\n",
    "# X_rfe = rfe.fit_transform(X_train,y_train)  \n",
    "# #Fitting the data to model\n",
    "# model.fit(X_rfe,y_train)              \n",
    "# temp = pd.Series(rfe.support_,index = cols)\n",
    "# selected_features_rfe = temp[temp==True].index\n",
    "# print(selected_features_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Trained model in {:.4f} seconds\",format(end - start))\n",
    "\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n",
    "    return f1_score(target.values, y_pred, pos_label=1)\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    print (\"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "    print (\"F1 score for test set: {:.4f}.\".format(predict_labels(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf1 = DecisionTreeClassifier()\n",
    "\n",
    "# cf3 = SVC()\n",
    "# cf2 = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "# cf2 = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())\n",
    "\n",
    "# n_estimators = number of trees in the foreset\n",
    "# max_features = max number of features considered for splitting a node\n",
    "# max_depth = max number of levels in each decision tree\n",
    "# min_samples_split = min number of data points placed in a node before the node is split\n",
    "# min_samples_leaf = min number of data points allowed in a leaf node\n",
    "# bootstrap = method for sampling data points (with or without replacement)\n",
    "# Method of selecting samples for training each tree\n",
    "#'max_features': ['auto'],\n",
    "#'criterion' :['gini']\n",
    "\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# parameters = {\n",
    "#     # Number of trees in random forest\n",
    "#     'n_estimators'      : [350,400,450],\n",
    "#     # Maximum number of levels in tree\n",
    "#     'max_depth'         : [12, 18, 30, 48, 78],\n",
    "#     'random_state'      : [0],\n",
    "#     # Number of features to consider at every split\n",
    "#     'max_features': ['auto', 'sqrt'],\n",
    "#     # Minimum number of samples required to split a node\n",
    "#     'min_samples_split' : [2, 5, 10],\n",
    "#     # Minimum number of samples required at each leaf node\n",
    "#     'min_samples_leaf' : [1, 2, 4]\n",
    "\n",
    "# }\n",
    "\n",
    "ideal = {\n",
    "    'max_depth': 48, \n",
    "    'max_features': 'auto', \n",
    "    'min_samples_leaf': 2, \n",
    "    'min_samples_split': 2, \n",
    "    'n_estimators': 400, \n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(**ideal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier()\n",
    "# clf = clf.fit(X_train, y_train)\n",
    "# fti = clf.feature_importances_\n",
    "# for i, feat in enumerate(X_train.columns):\n",
    "#     print('\\t{0:20s} : {1:>.6f}'.format(feat, fti[i]))\n",
    "\n",
    "# model = SelectFromModel(clf, prefit=True)\n",
    "# train_new = model.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = GridSearchCV(RandomForestClassifier(), parameters, cv=5, verbose=2, n_jobs=2)\n",
    "# clf.fit(train_new, y_train)\n",
    "\n",
    "# print(clf.score(train_new, y_train))\n",
    "# print(clf.best_params_)\n",
    "# print(zip(correct_train[predictor].columns, fti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### # Use the random grid to search for best hyperparameters\n",
    "# # First create the base model to tune\n",
    "# rf = RandomForestClassifier()\n",
    "# # Random search of parameters, using 3 fold cross validation, \n",
    "# # search across 100 different combinations, and use all available cores\n",
    "# #rf_random = RandomizedSearchCV(estimator = rf, param_distributions = parameters, cv = 10, verbose=2, n_jobs = 2)\n",
    "# # Fit the random search model\n",
    "# rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a RandomForestClassifier using a training set size of 950000. . .\n",
      "Trained model in {:.4f} seconds 2915.4017832279205\n"
     ]
    }
   ],
   "source": [
    "train_predict(rf, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "#predict_labels(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan = pd.DataFrame({'loan_status': y_pred[:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([id_test,loan], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
