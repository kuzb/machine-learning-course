{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier,GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pprint import pprint\n",
    "'''\n",
    "https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n",
    "https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./loansTest.csv\")\n",
    "train = pd.read_csv(\"./loansTrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature columns\n",
    "feature_cols = list(train.columns[:-1])\n",
    "# extract target column 'loan_status'\n",
    "target_col = train.columns[-1]\n",
    "\n",
    "# separate the data into feature data and target data\n",
    "X_all = train[feature_cols]\n",
    "y_all = train[target_col]\n",
    "\n",
    "# extract feature columns\n",
    "feature_cols = list(test.columns[1:])\n",
    "# extract target column 'loan_status'\n",
    "target_col = test.columns[0]\n",
    "\n",
    "# separate the data into feature data and target data\n",
    "X_test = test[feature_cols]\n",
    "id_test = test[target_col]\n",
    "\n",
    "X_all = pd.concat([X_all, X_test], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(X):\n",
    "    ''' Converts categorical variables into dummy variables. '''\n",
    "    \n",
    "    # Initialize new output DataFrame\n",
    "    output = pd.DataFrame(index = X.index)\n",
    "\n",
    "    # Investigate each feature column for the data\n",
    "    for col, col_data in X.iteritems():\n",
    "        print(col)\n",
    "        # If data type is categorical, convert to dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            # Example: 'school' => 'school_GP' and 'school_MS'\n",
    "            col_data = pd.get_dummies(col_data, prefix = col)  \n",
    "        \n",
    "        # Collect the revised columns\n",
    "        output = output.join(col_data)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all missing values unemployed\n",
    "# X_all['emp_title'].fillna(\"unemployed\",inplace=True)\n",
    "\n",
    "# # all titles lower case\n",
    "# X_all['emp_title'].str.lower()\n",
    "\n",
    "# # Replace non-alphanumeric\n",
    "# X_all['emp_title'] = X_all['emp_title'].astype(str).replace('[^a-zA-Z0-9 ]', '', regex=True)\n",
    "\n",
    "\n",
    "# # fix spelling mistakes\n",
    "# X_all['emp_title'].apply(lambda txt: ''.join(TextBlob(txt).correct()))\n",
    "\n",
    "# frequent_titles = pd.Series(' '.join(X_all['emp_title']).lower().split()).value_counts()[:200]\n",
    "# arr = frequent_titles.index.to_numpy()\n",
    "\n",
    "# to_be_ignored = ['the', 'of', 'and', '&', 'city', '-', '/','sr.', 'sr','inc.']\n",
    "\n",
    "# for ingnore in to_be_ignored:\n",
    "#     arr = np.delete(arr, np.argwhere(arr == ingnore))\n",
    "\n",
    "# whole = '.*|.*'.join(arr)\n",
    "# whole = '^(.*' + whole + '.*)'\n",
    "\n",
    "\n",
    "# arr = map(lambda x:[x, re.compile('.*'+ x +'.*')], arr )\n",
    "    \n",
    "# #arr = set(arr)\n",
    "# #iscomedy = arr.issubset\n",
    "# #temp = X_all['emp_title'].str.contains(arr, regex=True)\n",
    "\n",
    "# for r in arr:\n",
    "#     print(r)\n",
    "#     X_all['emp_title'] = X_all['emp_title'].str.replace(r[1], r[0], regex=True)\n",
    "    \n",
    "# X_all['emp_title'] = X_all['emp_title'].str.replace(re.compile(whole), '', regex=True)\n",
    "\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "X_all = X_all[X_all.columns[X_all.isnull().mean() < 0.7]]\n",
    "\n",
    "# replace null with zeros\n",
    "for col in X_all.columns:\n",
    "    # Replacing the missing values with the maximum occurred value \n",
    "    X_all[col].fillna(X_all[col].value_counts().idxmax(), inplace=True)\n",
    "    #X_all[col].fillna(0,inplace=True) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature removel\n",
    "X_all.drop(['emp_title', 'zip_code', 'addr_state', 'earliest_cr_line'], axis=1, inplace=True)\n",
    "# feature removel\n",
    "# X_all.drop(['zip_code', 'dti', 'avg_cur_bal','application_type','emp_title','chargeoff_within_12_mths','acc_now_delinq', 'delinq_amnt','percent_bc_gt_75','num_tl_120dpd_2m','num_tl_30dpd','num_tl_90g_dpd_24m','tax_liens'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_amnt\n",
      "term\n",
      "grade\n",
      "emp_length\n",
      "home_ownership\n",
      "annual_inc\n",
      "verification_status\n",
      "purpose\n",
      "dti\n",
      "delinq_2yrs\n",
      "inq_last_6mths\n",
      "open_acc\n",
      "pub_rec\n",
      "revol_bal\n",
      "revol_util\n",
      "collections_12_mths_ex_med\n",
      "application_type\n",
      "acc_now_delinq\n",
      "tot_coll_amt\n",
      "tot_cur_bal\n",
      "total_rev_hi_lim\n",
      "acc_open_past_24mths\n",
      "avg_cur_bal\n",
      "bc_open_to_buy\n",
      "bc_util\n",
      "chargeoff_within_12_mths\n",
      "delinq_amnt\n",
      "mo_sin_old_rev_tl_op\n",
      "mo_sin_rcnt_rev_tl_op\n",
      "mo_sin_rcnt_tl\n",
      "mort_acc\n",
      "mths_since_recent_bc\n",
      "mths_since_recent_inq\n",
      "num_accts_ever_120_pd\n",
      "num_actv_bc_tl\n",
      "num_actv_rev_tl\n",
      "num_bc_sats\n",
      "num_bc_tl\n",
      "num_il_tl\n",
      "num_op_rev_tl\n",
      "num_rev_accts\n",
      "num_rev_tl_bal_gt_0\n",
      "num_sats\n",
      "num_tl_120dpd_2m\n",
      "num_tl_30dpd\n",
      "num_tl_90g_dpd_24m\n",
      "num_tl_op_past_12m\n",
      "pct_tl_nvr_dlq\n",
      "percent_bc_gt_75\n",
      "pub_rec_bankruptcies\n",
      "tax_liens\n",
      "tot_hi_cred_lim\n",
      "total_bal_ex_mort\n",
      "total_il_high_credit_limit\n"
     ]
    }
   ],
   "source": [
    "# X_all = pd.get_dummies(X_all)\n",
    "X_all['term'] = X_all['term'].replace(['36 months', '60 months'], [0, 1])\n",
    "X_all['term'] = X_all['term'].replace([' 36 months', ' 60 months'], [0, 1])\n",
    "X_all['verification_status'] = pd.get_dummies(X_all['verification_status'], prefix = 'verification_status')  \n",
    "X_all['purpose'] = pd.get_dummies(X_all['purpose'], prefix = 'purpose')  \n",
    "X_all['application_type'] = pd.get_dummies(X_all['application_type'], prefix = 'application_type')  \n",
    "\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "X, X_test = X_all.iloc[:1000000,:], X_all.iloc[1000000:,:]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_all, train_size=0.05, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Selection\n",
    "Statistical tests can be used to select those features that have the strongest relationship with the output variable. Chi-squared (chiÂ²) statistical test is being used to select 10 of the best features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "fit = bestfeatures.fit(X_train,y_train)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X_train.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importence\n",
    "Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix with Heatmap\n",
    "Correlation states how the features are related to each other or the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "#get correlations of each features in dataset\n",
    "corrmat = train.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "g=sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_selector import FeatureSelector\n",
    "# Features are in train and labels are in train_labels\n",
    "fs = FeatureSelector(data = train, labels = train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collinear Features\n",
    "Collinear features are features that are highly correlated with one another. In machine learning, these lead to decreased generalization performance on the test set due to high variance and less model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collinear Features\n",
    "fs.identify_collinear(correlation_threshold = 0.98)\n",
    "fs.plot_collinear()\n",
    "# list of collinear features to remove\n",
    "collinear_features = fs.ops['collinear']\n",
    "# dataframe of collinear features\n",
    "fs.record_collinear.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Importence Features\n",
    "The identify_zero_importance function finds features that have zero importance according to a gradient boosting machine (GBM) learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the appropriate parameters\n",
    "fs.identify_zero_importance(task = 'classification', \n",
    "                            eval_metric = 'auc', \n",
    "                            n_iterations = 10, \n",
    "                             early_stopping = True)\n",
    "# list of zero importance features\n",
    "zero_importance_features = fs.ops['zero_importance']\n",
    "# plot the feature importances\n",
    "fs.plot_feature_importances(threshold = 0.99, plot_n = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.identify_low_importance(cumulative_importance = 0.99)\n",
    "fs.feature_importances.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Trained model in {:.4f} seconds\",format(end - start))\n",
    "\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n",
    "    return f1_score(target.values, y_pred, pos_label=1)\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    print (\"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "    print (\"F1 score for test set: {:.4f}.\".format(predict_labels(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf1 = DecisionTreeClassifier()\n",
    "\n",
    "cf3 = SVC()\n",
    "cf2 = RandomForestClassifier(max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())\n",
    "\n",
    "# n_estimators = number of trees in the foreset\n",
    "# max_features = max number of features considered for splitting a node\n",
    "# max_depth = max number of levels in each decision tree\n",
    "# min_samples_split = min number of data points placed in a node before the node is split\n",
    "# min_samples_leaf = min number of data points allowed in a leaf node\n",
    "# bootstrap = method for sampling data points (with or without replacement)\n",
    "# Method of selecting samples for training each tree\n",
    "#'max_features': ['auto'],\n",
    "#'criterion' :['gini']\n",
    "\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "parameters = {\n",
    "    # Number of trees in random forest\n",
    "    'n_estimators'      : [350,400,450],\n",
    "    # Maximum number of levels in tree\n",
    "    'max_depth'         : [12, 18, 30, 48, 78],\n",
    "    'random_state'      : [0],\n",
    "    # Number of features to consider at every split\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    # Minimum number of samples required to split a node\n",
    "    'min_samples_split' : [2, 5, 10],\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    'min_samples_leaf' : [1, 2, 4]\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloan_amnt            : 0.034136\n",
      "\tterm                 : 0.016625\n",
      "\tgrade_A              : 0.013526\n",
      "\tgrade_B              : 0.008147\n",
      "\tgrade_C              : 0.005194\n",
      "\tgrade_D              : 0.006459\n",
      "\tgrade_E              : 0.007866\n",
      "\tgrade_F              : 0.004676\n",
      "\tgrade_G              : 0.001673\n",
      "\temp_length_1 year    : 0.002930\n",
      "\temp_length_10+ years : 0.005274\n",
      "\temp_length_2 years   : 0.003256\n",
      "\temp_length_3 years   : 0.003234\n",
      "\temp_length_4 years   : 0.002753\n",
      "\temp_length_5 years   : 0.002703\n",
      "\temp_length_6 years   : 0.002452\n",
      "\temp_length_7 years   : 0.002488\n",
      "\temp_length_8 years   : 0.002540\n",
      "\temp_length_9 years   : 0.002295\n",
      "\temp_length_< 1 year  : 0.003386\n",
      "\thome_ownership_ANY   : 0.000002\n",
      "\thome_ownership_MORTGAGE : 0.004158\n",
      "\thome_ownership_NONE  : 0.000008\n",
      "\thome_ownership_OTHER : 0.000044\n",
      "\thome_ownership_OWN   : 0.003149\n",
      "\thome_ownership_RENT  : 0.004611\n",
      "\tannual_inc           : 0.035316\n",
      "\tverification_status  : 0.005096\n",
      "\tpurpose              : 0.000691\n",
      "\tdti                  : 0.041694\n",
      "\tdelinq_2yrs          : 0.007124\n",
      "\tinq_last_6mths       : 0.011227\n",
      "\topen_acc             : 0.020637\n",
      "\tpub_rec              : 0.005421\n",
      "\trevol_bal            : 0.034283\n",
      "\trevol_util           : 0.035130\n",
      "\tcollections_12_mths_ex_med : 0.001342\n",
      "\tapplication_type     : 0.001352\n",
      "\tacc_now_delinq       : 0.000303\n",
      "\ttot_coll_amt         : 0.009670\n",
      "\ttot_cur_bal          : 0.030174\n",
      "\ttotal_rev_hi_lim     : 0.030902\n",
      "\tacc_open_past_24mths : 0.021682\n",
      "\tavg_cur_bal          : 0.031732\n",
      "\tbc_open_to_buy       : 0.032872\n",
      "\tbc_util              : 0.032054\n",
      "\tchargeoff_within_12_mths : 0.000755\n",
      "\tdelinq_amnt          : 0.000503\n",
      "\tmo_sin_old_rev_tl_op : 0.034052\n",
      "\tmo_sin_rcnt_rev_tl_op : 0.022887\n",
      "\tmo_sin_rcnt_tl       : 0.021166\n",
      "\tmort_acc             : 0.012625\n",
      "\tmths_since_recent_bc : 0.026991\n",
      "\tmths_since_recent_inq : 0.022226\n",
      "\tnum_accts_ever_120_pd : 0.007597\n",
      "\tnum_actv_bc_tl       : 0.014672\n",
      "\tnum_actv_rev_tl      : 0.016351\n",
      "\tnum_bc_sats          : 0.016359\n",
      "\tnum_bc_tl            : 0.021054\n",
      "\tnum_il_tl            : 0.023497\n",
      "\tnum_op_rev_tl        : 0.017709\n",
      "\tnum_rev_accts        : 0.023713\n",
      "\tnum_rev_tl_bal_gt_0  : 0.016033\n",
      "\tnum_sats             : 0.018887\n",
      "\tnum_tl_120dpd_2m     : 0.000104\n",
      "\tnum_tl_30dpd         : 0.000245\n",
      "\tnum_tl_90g_dpd_24m   : 0.002842\n",
      "\tnum_tl_op_past_12m   : 0.015056\n",
      "\tpct_tl_nvr_dlq       : 0.018690\n",
      "\tpercent_bc_gt_75     : 0.016981\n",
      "\tpub_rec_bankruptcies : 0.004124\n",
      "\ttax_liens            : 0.002242\n",
      "\ttot_hi_cred_lim      : 0.031956\n",
      "\ttotal_bal_ex_mort    : 0.030439\n",
      "\ttotal_il_high_credit_limit : 0.027960\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = RandomForestClassifier(n_estimators=320)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "fti = clf.feature_importances_\n",
    "for i, feat in enumerate(X_train.columns):\n",
    "    print('\\t{0:20s} : {1:>.6f}'.format(feat, fti[i]))\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "train_new = model.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=2)]: Done 150 out of 150 | elapsed: 33.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82678\n",
      "{'max_depth': 12, 'n_estimators': 340, 'random_state': 0}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'correct_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b06325160d46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'correct_train' is not defined"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(RandomForestClassifier(), parameters, cv=5, verbose=2, n_jobs=2)\n",
    "clf.fit(train_new, y_train)\n",
    "\n",
    "print(clf.score(train_new, y_train))\n",
    "print(clf.best_params_)\n",
    "print(zip(correct_train[predictor].columns, fti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### # Use the random grid to search for best hyperparameters\n",
    "# # First create the base model to tune\n",
    "# rf = RandomForestClassifier()\n",
    "# # Random search of parameters, using 3 fold cross validation, \n",
    "# # search across 100 different combinations, and use all available cores\n",
    "# #rf_random = RandomizedSearchCV(estimator = rf, param_distributions = parameters, cv = 10, verbose=2, n_jobs = 2)\n",
    "# # Fit the random search model\n",
    "# rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict(cf2, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cf2.predict(X_test)\n",
    "\n",
    "#predict_labels(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan = pd.DataFrame({'loan_status': y_pred[:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([id_test,loan], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
